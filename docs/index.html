---

title: Fraud POC


keywords: fastai
sidebar: home_sidebar



nb_path: "index.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: index.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a prototype/proof-of-concept of a ML solution for Fraud Detection for e-commerce,
completely built in Python and backed by Dask to parallelize data processing and model training
and hopeit.engine to "productionize" training pipeline and prediction service as microservices.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Author: Leo Smerling</p>
<p>LinkedIn: <a href="https://www.linkedin.com/in/leosmerling/">https://www.linkedin.com/in/leosmerling/</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://dask.org/">Dask</a> is a distributed processing engine for Python that can be used to process high data loads in distributed environments, such a Dask cluster. It has APIs built on top of popular Numpy and Pandas libraries.</p>
<p><a href="https://github.com/hopeit-git/hopeit.engine">hopeit.engine</a>: is an (upcoming) open-source library that I am contributing to, that enables to quickly develop microservices in Python. hopeit.engine is built on top of aiohttp to provide API endpoints and async processing, and also provides distributed processing of streaming events using <a href="https://redis.io/topics/streams-intro">Redis Streams</a>. Streaming data, authorization, logging, metrics and tracking/tracing are added to your microservice out of the box.</p>
<p>To enable development, testing and working with the data in an unified environment I use <a href="https://github.com/fastai/nbdev">nbdev</a>. nbdev allows to explore data, create the services and test using Jupyter Notebooks. hopeit.engine and Dask plays well also with Juypyter notebooks, so the whole pipeline and prediction service can be developed and tested from Jupyter.</p>
<p>This repo shows and end-to-end example of a Fraud Detection system consisting of:</p>
<ul>
<li>Data preprocessing and partitioning (using Dasks Dataframes API)</li>
<li>Feature calculation (using Dasks Dataframes API)</li>
<li>Preparing data for model training (using Dasks Dataframes API)</li>
<li>Training a model (distributed Dask XGBoost)</li>
<li>Preparing data to serve predictions (using Dask + Redis)</li>
<li>Prepare and run a microservice to orchestrate and monitor the data + training pipeline (using hopeit.engine)</li>
<li>Prepare and run a microservice to predict fraud on new orders (using hopeit.engine) </li>
</ul>
<p><strong>DISCLAIMER</strong>: The objective of this project is to quickly show an example on how Data + Feature Extraction +
Model Trainig + Prediction can be developed and prepared for production. The data used for this example
is randomly generated orders, and neither the features selected and model parameteres were optimized
given the nature of data used. The intention is to give an overview of the tools and the approach to quickstart a project that could evolve into a mature state by improving each one of its pieces.</p>
<h3 id="Getting-started">Getting started<a class="anchor-link" href="#Getting-started"> </a></h3><p>(Feel free to report issues if you find the procedure below not working, I've tested only in a Linux environment)</p>
<ul>
<li>I recommend to install <a href="https://docs.anaconda.com/anaconda/install/">Anaconda</a> (virtualenv can be used also -- not tested --)</li>
</ul>
<ul>
<li>Create a conda environment, activate and install jupyterlab, nbdev and dask
<pre><code>conda create -n fraud-poc python=3.7
conda activate fraud-poc
conda install jupyterlab
conda install -c conda-forge dask graphviz python-graphviz 
pip install nbdev
nbdev_install_git_hooks</code></pre>
</li>
</ul>
<ul>
<li>Install hopeit.engine from provided library (preview version, do not use in production):
<pre><code>cd install
source install-hopeit-engine.sh
cd ..</code></pre>
</li>
</ul>
<ul>
<li>Finally install this project and dependencies in development mode
<pre><code>pip install -e .</code></pre>
</li>
</ul>
<ul>
<li><p>In order to run the microservices (optional) Redis is required. You can run redis for development, from the provided docker configuration:</p>

<pre><code>cd docker
pip install docker-compose
docker-compose up -d redis
cd ..</code></pre>
</li>
<li><p>You can start a Dask cluster using docker or locally:</p>
</li>
</ul>
<p>Docker:</p>

<pre><code>docker-compose up dask-scheduler
docker-compose up dask-worker</code></pre>
<p>Locally:</p>

<pre><code>dask-scheduler
dask-worker tcp://localhost:8786 --memory-limit 2GB</code></pre>
<ul>
<li>Create a folder to store data
<pre><code>mkdir data</code></pre>
(location can be changed from config files)</li>
</ul>
<ul>
<li>To open, visualize and edit notebooks run from the root folder of the project
<pre><code>jupyter lab</code></pre>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Overview:">Overview:<a class="anchor-link" href="#Overview:"> </a></h3><ul>
<li><p>Notebooks prefixed from 00<em> to 09</em> are created for each component/event of the pipeline and prediction service. Check each notebook for a brief description of what they do:</p>
</li>
<li><p>Cells marked with #export, will generate a python file inside fraud_poc/ folder that can be executed by hopeit.engine</p>
</li>
<li><p>To generate/update the code, run <code>nbdev_build_lib</code> (no need to do it if you haven't change the code, modules are already generated in the repo)</p>
</li>
<li><p>Usually the last cells of the notebooks, are a test case that can be run locally and invoke the generated file, gather data and do some checks/analysis. I saved the notebooks with the outputs so you can visualize some examples without needing to install anything.</p>
</li>
<li><p>Inside config/ folder there are configuration files to run two microservices:</p>
<ul>
<li><p><code>training_pipeline.json</code> and <code>openapi-training.json</code> describe the service to run data preparation and training pipeline using hopeit.engine</p>
</li>
<li><p><code>fraud-service.json</code> and <code>openapi-service.json</code> configure a service to perform real-time predictions on new orders based on the training model and aggregated data</p>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Training-Pipeline">Training Pipeline<a class="anchor-link" href="#Training-Pipeline"> </a></h4><p>As a result of configuration in <code>config/training-config.json</code> plus implemented Python modules generated from Notebooks 00* to 07, the following training pipeline is implemented, where event (green) is notified by the previous step using streams (blueish):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pipeline_diagram</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_svg output_subarea output_execute_result">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.42.3 (20191010.1750)
 -->
<!-- Title: %3 Pages: 1 -->
<svg width="396pt" height="737pt"
 viewBox="0.00 0.00 396.40 736.60" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 732.6)">
<title>%3</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-732.6 392.4,-732.6 392.4,4 -4,4"/>
<!-- submit_training_pipeline.POST -->
<g id="node1" class="node">
<title>submit_training_pipeline.POST</title>
<ellipse fill="#aaccaa" stroke="black" cx="111.65" cy="-726.8" rx="1.8" ry="1.8"/>
</g>
<!-- submit_training_pipeline -->
<g id="node3" class="node">
<title>submit_training_pipeline</title>
<polygon fill="#aaccaa" stroke="black" points="189.9,-689 33.39,-689 33.39,-653 189.9,-653 189.9,-689"/>
<text text-anchor="middle" x="111.65" y="-666.8" font-family="Times,serif" font-size="14.00">submit_training_pipeline</text>
</g>
<!-- submit_training_pipeline.POST&#45;&gt;submit_training_pipeline -->
<g id="edge1" class="edge">
<title>submit_training_pipeline.POST&#45;&gt;submit_training_pipeline</title>
<path fill="none" stroke="black" d="M111.65,-724.87C111.65,-721.64 111.65,-710.55 111.65,-699.35"/>
<polygon fill="black" stroke="black" points="115.15,-699.26 111.65,-689.26 108.15,-699.26 115.15,-699.26"/>
</g>
<!-- fraud_poc.training.submit_training_pipeline -->
<g id="node2" class="node">
<title>fraud_poc.training.submit_training_pipeline</title>
<path fill="#aaaaff" stroke="black" d="M98.69,-580.5C98.69,-580.5 292.6,-580.5 292.6,-580.5 298.6,-580.5 304.6,-586.5 304.6,-592.5 304.6,-592.5 304.6,-604.5 304.6,-604.5 304.6,-610.5 298.6,-616.5 292.6,-616.5 292.6,-616.5 98.69,-616.5 98.69,-616.5 92.69,-616.5 86.69,-610.5 86.69,-604.5 86.69,-604.5 86.69,-592.5 86.69,-592.5 86.69,-586.5 92.69,-580.5 98.69,-580.5"/>
<text text-anchor="middle" x="195.65" y="-595.74" font-family="Times,serif" font-size="10.00">→ fraud_poc.training.submit_training_pipeline →</text>
</g>
<!-- data.preprocess -->
<g id="node7" class="node">
<title>data.preprocess</title>
<polygon fill="#aaccaa" stroke="black" points="246.83,-544 144.47,-544 144.47,-508 246.83,-508 246.83,-544"/>
<text text-anchor="middle" x="195.65" y="-521.8" font-family="Times,serif" font-size="14.00">data.preprocess</text>
</g>
<!-- fraud_poc.training.submit_training_pipeline&#45;&gt;data.preprocess -->
<g id="edge2" class="edge">
<title>fraud_poc.training.submit_training_pipeline&#45;&gt;data.preprocess</title>
<path fill="none" stroke="black" d="M195.65,-580.43C195.65,-572.67 195.65,-563.28 195.65,-554.56"/>
<polygon fill="black" stroke="black" points="199.15,-554.4 195.65,-544.4 192.15,-554.4 199.15,-554.4"/>
</g>
<!-- submit_training_pipeline&#45;&gt;fraud_poc.training.submit_training_pipeline -->
<g id="edge3" class="edge">
<title>submit_training_pipeline&#45;&gt;fraud_poc.training.submit_training_pipeline</title>
<path fill="none" stroke="black" d="M131.98,-652.93C142.63,-643.99 155.84,-632.91 167.47,-623.15"/>
<polygon fill="black" stroke="black" points="169.8,-625.76 175.21,-616.65 165.3,-620.4 169.8,-625.76"/>
</g>
<!-- data.make&#45;sample&#45;data.POST -->
<g id="node4" class="node">
<title>data.make&#45;sample&#45;data.POST</title>
<ellipse fill="#aaccaa" stroke="black" cx="280.65" cy="-726.8" rx="1.8" ry="1.8"/>
</g>
<!-- data.make&#45;sample&#45;data -->
<g id="node5" class="node">
<title>data.make&#45;sample&#45;data</title>
<polygon fill="#aaccaa" stroke="black" points="353.58,-689 207.71,-689 207.71,-653 353.58,-653 353.58,-689"/>
<text text-anchor="middle" x="280.65" y="-666.8" font-family="Times,serif" font-size="14.00">data.make&#45;sample&#45;data</text>
</g>
<!-- data.make&#45;sample&#45;data.POST&#45;&gt;data.make&#45;sample&#45;data -->
<g id="edge5" class="edge">
<title>data.make&#45;sample&#45;data.POST&#45;&gt;data.make&#45;sample&#45;data</title>
<path fill="none" stroke="black" d="M280.65,-724.87C280.65,-721.64 280.65,-710.55 280.65,-699.35"/>
<polygon fill="black" stroke="black" points="284.15,-699.26 280.65,-689.26 277.15,-699.26 284.15,-699.26"/>
</g>
<!-- data.make&#45;sample&#45;data&#45;&gt;fraud_poc.training.submit_training_pipeline -->
<g id="edge4" class="edge">
<title>data.make&#45;sample&#45;data&#45;&gt;fraud_poc.training.submit_training_pipeline</title>
<path fill="none" stroke="black" d="M260.07,-652.93C249.29,-643.99 235.92,-632.91 224.16,-623.15"/>
<polygon fill="black" stroke="black" points="226.26,-620.34 216.33,-616.65 221.79,-625.73 226.26,-620.34"/>
</g>
<!-- fraud_poc.training.data.preprocess -->
<g id="node6" class="node">
<title>fraud_poc.training.data.preprocess</title>
<path fill="#aaaaff" stroke="black" d="M118,-435.5C118,-435.5 273.29,-435.5 273.29,-435.5 279.29,-435.5 285.29,-441.5 285.29,-447.5 285.29,-447.5 285.29,-459.5 285.29,-459.5 285.29,-465.5 279.29,-471.5 273.29,-471.5 273.29,-471.5 118,-471.5 118,-471.5 112,-471.5 106,-465.5 106,-459.5 106,-459.5 106,-447.5 106,-447.5 106,-441.5 112,-435.5 118,-435.5"/>
<text text-anchor="middle" x="195.65" y="-450.74" font-family="Times,serif" font-size="10.00">→ fraud_poc.training.data.preprocess →</text>
</g>
<!-- data.feature&#45;calc -->
<g id="node9" class="node">
<title>data.feature&#45;calc</title>
<polygon fill="#aaccaa" stroke="black" points="250.01,-399 141.28,-399 141.28,-363 250.01,-363 250.01,-399"/>
<text text-anchor="middle" x="195.65" y="-376.8" font-family="Times,serif" font-size="14.00">data.feature&#45;calc</text>
</g>
<!-- fraud_poc.training.data.preprocess&#45;&gt;data.feature&#45;calc -->
<g id="edge6" class="edge">
<title>fraud_poc.training.data.preprocess&#45;&gt;data.feature&#45;calc</title>
<path fill="none" stroke="black" d="M195.65,-435.43C195.65,-427.67 195.65,-418.28 195.65,-409.56"/>
<polygon fill="black" stroke="black" points="199.15,-409.4 195.65,-399.4 192.15,-409.4 199.15,-409.4"/>
</g>
<!-- data.preprocess&#45;&gt;fraud_poc.training.data.preprocess -->
<g id="edge7" class="edge">
<title>data.preprocess&#45;&gt;fraud_poc.training.data.preprocess</title>
<path fill="none" stroke="black" d="M195.65,-507.93C195.65,-500.17 195.65,-490.78 195.65,-482.06"/>
<polygon fill="black" stroke="black" points="199.15,-481.9 195.65,-471.9 192.15,-481.9 199.15,-481.9"/>
</g>
<!-- fraud_poc.training.data.feature_calc -->
<g id="node8" class="node">
<title>fraud_poc.training.data.feature_calc</title>
<path fill="#aaaaff" stroke="black" d="M114.96,-290.5C114.96,-290.5 276.34,-290.5 276.34,-290.5 282.34,-290.5 288.34,-296.5 288.34,-302.5 288.34,-302.5 288.34,-314.5 288.34,-314.5 288.34,-320.5 282.34,-326.5 276.34,-326.5 276.34,-326.5 114.96,-326.5 114.96,-326.5 108.96,-326.5 102.96,-320.5 102.96,-314.5 102.96,-314.5 102.96,-302.5 102.96,-302.5 102.96,-296.5 108.96,-290.5 114.96,-290.5"/>
<text text-anchor="middle" x="195.65" y="-305.74" font-family="Times,serif" font-size="10.00">→ fraud_poc.training.data.feature_calc →</text>
</g>
<!-- data.training&#45;data -->
<g id="node11" class="node">
<title>data.training&#45;data</title>
<polygon fill="#aaccaa" stroke="black" points="189.99,-254 75.3,-254 75.3,-218 189.99,-218 189.99,-254"/>
<text text-anchor="middle" x="132.65" y="-231.8" font-family="Times,serif" font-size="14.00">data.training&#45;data</text>
</g>
<!-- fraud_poc.training.data.feature_calc&#45;&gt;data.training&#45;data -->
<g id="edge8" class="edge">
<title>fraud_poc.training.data.feature_calc&#45;&gt;data.training&#45;data</title>
<path fill="none" stroke="black" d="M180.39,-290.43C172.77,-281.9 163.4,-271.41 154.98,-261.99"/>
<polygon fill="black" stroke="black" points="157.47,-259.52 148.19,-254.4 152.25,-264.19 157.47,-259.52"/>
</g>
<!-- data.prepare&#45;db -->
<g id="node13" class="node">
<title>data.prepare&#45;db</title>
<polygon fill="#aaccaa" stroke="black" points="330.59,-254 226.7,-254 226.7,-218 330.59,-218 330.59,-254"/>
<text text-anchor="middle" x="278.65" y="-231.8" font-family="Times,serif" font-size="14.00">data.prepare&#45;db</text>
</g>
<!-- fraud_poc.training.data.feature_calc&#45;&gt;data.prepare&#45;db -->
<g id="edge9" class="edge">
<title>fraud_poc.training.data.feature_calc&#45;&gt;data.prepare&#45;db</title>
<path fill="none" stroke="black" d="M215.74,-290.43C226.26,-281.49 239.32,-270.41 250.8,-260.65"/>
<polygon fill="black" stroke="black" points="253.1,-263.29 258.45,-254.15 248.56,-257.96 253.1,-263.29"/>
</g>
<!-- data.feature&#45;calc&#45;&gt;fraud_poc.training.data.feature_calc -->
<g id="edge10" class="edge">
<title>data.feature&#45;calc&#45;&gt;fraud_poc.training.data.feature_calc</title>
<path fill="none" stroke="black" d="M195.65,-362.93C195.65,-355.17 195.65,-345.78 195.65,-337.06"/>
<polygon fill="black" stroke="black" points="199.15,-336.9 195.65,-326.9 192.15,-336.9 199.15,-336.9"/>
</g>
<!-- fraud_poc.training.data.training_data -->
<g id="node10" class="node">
<title>fraud_poc.training.data.training_data</title>
<path fill="#aaaaff" stroke="black" d="M12,-145.5C12,-145.5 177.29,-145.5 177.29,-145.5 183.29,-145.5 189.29,-151.5 189.29,-157.5 189.29,-157.5 189.29,-169.5 189.29,-169.5 189.29,-175.5 183.29,-181.5 177.29,-181.5 177.29,-181.5 12,-181.5 12,-181.5 6,-181.5 0,-175.5 0,-169.5 0,-169.5 0,-157.5 0,-157.5 0,-151.5 6,-145.5 12,-145.5"/>
<text text-anchor="middle" x="94.65" y="-160.74" font-family="Times,serif" font-size="10.00">→ fraud_poc.training.data.training_data →</text>
</g>
<!-- model.train -->
<g id="node15" class="node">
<title>model.train</title>
<polygon fill="#aaccaa" stroke="black" points="134.79,-109 54.5,-109 54.5,-73 134.79,-73 134.79,-109"/>
<text text-anchor="middle" x="94.65" y="-86.8" font-family="Times,serif" font-size="14.00">model.train</text>
</g>
<!-- fraud_poc.training.data.training_data&#45;&gt;model.train -->
<g id="edge11" class="edge">
<title>fraud_poc.training.data.training_data&#45;&gt;model.train</title>
<path fill="none" stroke="black" d="M94.65,-145.43C94.65,-137.67 94.65,-128.28 94.65,-119.56"/>
<polygon fill="black" stroke="black" points="98.15,-119.4 94.65,-109.4 91.15,-119.4 98.15,-119.4"/>
</g>
<!-- data.training&#45;data&#45;&gt;fraud_poc.training.data.training_data -->
<g id="edge12" class="edge">
<title>data.training&#45;data&#45;&gt;fraud_poc.training.data.training_data</title>
<path fill="none" stroke="black" d="M123.45,-217.93C119.08,-209.83 113.76,-199.96 108.88,-190.91"/>
<polygon fill="black" stroke="black" points="111.85,-189.04 104.02,-181.9 105.69,-192.36 111.85,-189.04"/>
</g>
<!-- fraud_poc.training.data.prepare_db -->
<g id="node12" class="node">
<title>fraud_poc.training.data.prepare_db</title>
<path fill="#aaaaff" stroke="black" d="M218.89,-145.5C218.89,-145.5 376.4,-145.5 376.4,-145.5 382.4,-145.5 388.4,-151.5 388.4,-157.5 388.4,-157.5 388.4,-169.5 388.4,-169.5 388.4,-175.5 382.4,-181.5 376.4,-181.5 376.4,-181.5 218.89,-181.5 218.89,-181.5 212.89,-181.5 206.89,-175.5 206.89,-169.5 206.89,-169.5 206.89,-157.5 206.89,-157.5 206.89,-151.5 212.89,-145.5 218.89,-145.5"/>
<text text-anchor="middle" x="297.65" y="-160.74" font-family="Times,serif" font-size="10.00">→ fraud_poc.training.data.prepare_db →</text>
</g>
<!-- data.prepare&#45;db&#45;&gt;fraud_poc.training.data.prepare_db -->
<g id="edge13" class="edge">
<title>data.prepare&#45;db&#45;&gt;fraud_poc.training.data.prepare_db</title>
<path fill="none" stroke="black" d="M283.24,-217.93C285.36,-210.08 287.92,-200.58 290.3,-191.77"/>
<polygon fill="black" stroke="black" points="293.73,-192.46 292.96,-181.9 286.97,-190.64 293.73,-192.46"/>
</g>
<!-- fraud_poc.training.model.train -->
<g id="node14" class="node">
<title>fraud_poc.training.model.train</title>
<path fill="#aaaaff" stroke="black" d="M25.05,-0.5C25.05,-0.5 164.24,-0.5 164.24,-0.5 170.24,-0.5 176.24,-6.5 176.24,-12.5 176.24,-12.5 176.24,-24.5 176.24,-24.5 176.24,-30.5 170.24,-36.5 164.24,-36.5 164.24,-36.5 25.05,-36.5 25.05,-36.5 19.05,-36.5 13.05,-30.5 13.05,-24.5 13.05,-24.5 13.05,-12.5 13.05,-12.5 13.05,-6.5 19.05,-0.5 25.05,-0.5"/>
<text text-anchor="middle" x="94.65" y="-15.74" font-family="Times,serif" font-size="10.00">→ fraud_poc.training.model.train →</text>
</g>
<!-- model.train&#45;&gt;fraud_poc.training.model.train -->
<g id="edge14" class="edge">
<title>model.train&#45;&gt;fraud_poc.training.model.train</title>
<path fill="none" stroke="black" d="M94.65,-72.93C94.65,-65.17 94.65,-55.78 94.65,-47.06"/>
<polygon fill="black" stroke="black" points="98.15,-46.9 94.65,-36.9 91.15,-46.9 98.15,-46.9"/>
</g>
</g>
</svg>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are two entry points:</p>
<ul>
<li>make_sample_data: endpoint to create sample data and trigger the pipeline steps.</li>
<li>submit_training_pipleine: endpoint to trigger pipeline steps from already existing data</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Inference-Service">Inference Service<a class="anchor-link" href="#Inference-Service"> </a></h4><p>As configured in <code>fraud-service.json</code> with events implementes in notebooks 08<em> to 09</em>, there is a resulting service with two endpoints:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">service_diagram</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_svg output_subarea output_execute_result">
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.42.3 (20191010.1750)
 -->
<!-- Title: %3 Pages: 1 -->
<svg width="208pt" height="84pt"
 viewBox="0.00 0.00 208.04 83.60" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 79.6)">
<title>%3</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-79.6 204.04,-79.6 204.04,4 -4,4"/>
<!-- live.predict.POST -->
<g id="node1" class="node">
<title>live.predict.POST</title>
<ellipse fill="#aaccaa" stroke="black" cx="39.68" cy="-73.8" rx="1.8" ry="1.8"/>
</g>
<!-- live.predict -->
<g id="node2" class="node">
<title>live.predict</title>
<polygon fill="#aaccaa" stroke="black" points="79.54,-36 -0.18,-36 -0.18,0 79.54,0 79.54,-36"/>
<text text-anchor="middle" x="39.68" y="-13.8" font-family="Times,serif" font-size="14.00">live.predict</text>
</g>
<!-- live.predict.POST&#45;&gt;live.predict -->
<g id="edge1" class="edge">
<title>live.predict.POST&#45;&gt;live.predict</title>
<path fill="none" stroke="black" d="M39.68,-71.87C39.68,-68.64 39.68,-57.55 39.68,-46.35"/>
<polygon fill="black" stroke="black" points="43.18,-46.26 39.68,-36.26 36.18,-46.26 43.18,-46.26"/>
</g>
<!-- test.find_orders.GET -->
<g id="node3" class="node">
<title>test.find_orders.GET</title>
<ellipse fill="#aaccaa" stroke="black" cx="148.68" cy="-73.8" rx="1.8" ry="1.8"/>
</g>
<!-- test.find_orders -->
<g id="node4" class="node">
<title>test.find_orders</title>
<polygon fill="#aaccaa" stroke="black" points="199.9,-36 97.47,-36 97.47,0 199.9,0 199.9,-36"/>
<text text-anchor="middle" x="148.68" y="-13.8" font-family="Times,serif" font-size="14.00">test.find_orders</text>
</g>
<!-- test.find_orders.GET&#45;&gt;test.find_orders -->
<g id="edge2" class="edge">
<title>test.find_orders.GET&#45;&gt;test.find_orders</title>
<path fill="none" stroke="black" d="M148.68,-71.87C148.68,-68.64 148.68,-57.55 148.68,-46.35"/>
<polygon fill="black" stroke="black" points="152.18,-46.26 148.68,-36.26 145.18,-46.26 152.18,-46.26"/>
</g>
</g>
</svg>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>live.predict: endpoint required to enter order information and return predictions and calculated feature values.</li>
<li>test.find_orders: it's a helper endpoint to find random generated orders to be used in this example</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Data-processing-and-training-pipeline">Data processing and training pipeline<a class="anchor-link" href="#Data-processing-and-training-pipeline"> </a></h3><ul>
<li>To run training pipeline service:</li>
</ul>

<pre><code>hopeit_server run --config-files=config/server.json,config/training-pipeline.json --api-file=config/openapi-training.json --start-streams --port=8020</code></pre>
<p>You should see a couple endpoints in <a href="http://localhost:8020/api/docs">http://localhost:8020/api/docs</a></p>
<ul>
<li>The first endpoint "Data: Make Sample Data" will run the whole data+training pipeline end to end if you click in <code>Try It</code>
<img src="/fraud_poc/docs/img/api01.png" alt=""></li>
</ul>
<p>1) <strong>create-sample-data</strong>: will create random orders in parquet format into folder <code>./data/raw/</code>. This dataset is partitioned by time periods, i.e. 1 file per 30-day batch in this example. Once this step is finished the end of the job will be notified using hopeit.engine streams funcionallity and the next job will take place once the event is consumed.</p>
<p>2) <strong>preprocess</strong>: reads data generated in previous step and creates new parquet files partitioned by customer_id and email, so aggregations on those two dimensions can be performed more efficiently later. Again, once the job is finished, the next step will be notified. This generated files also can be use for data analysis and feature discovering using Jupyter and Dask.</p>
<p>3) <strong>feature-calc</strong>: calculates aggregations on customer_ids and emails (i.e. accumulates most recent emails, ip_addrs, counts, order_amounts, etc) and stores a new data set of orders enriched with this extra information.</p>
<p>4) <strong>training-data</strong>: prepares data for training: obtain labels for the orders (in this POC <code>is_fraud</code> label field is just assigned using a combination of calculations with some randomness) and creates a more balanced dataset subsampling non-fraud cases, creates a validation set using more recent non-fraud and fraud labeled transactions. Next step is notified when data is ready. The dataset is shuffle randomly into N partitions (10 in the example) so training can be performed from each partition using fairly-balanced datasets.</p>
<p>5) <strong>train-model</strong>: trains an XGBoost model on sampled data using Dask distributed implementation. Validates model precision and recall using validation dataset and if validation passes a configured treshold, model is saved to be used in prediction service.</p>
<p>6) <strong>prepare-db</strong>: stores most recent customer_id and email features calculated in step 3) into a Redis database that can be used for real-time prediction service. (Notice that this data should be continuously updated on new orders but this is not provided in this POC)</p>
<p>Since data generation could be tedious, there is a second endpoint that allows to run just from step 02, assuming
you already have raw data:
<img src="/fraud_poc/docs/img/api02.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Fraud-prediction-service">Fraud prediction service<a class="anchor-link" href="#Fraud-prediction-service"> </a></h3><p>To run the live prediction service:</p>

<pre><code>hopeit_server run --config-files=config/server.json,config/fraud-service.json --api-file=config/openapi-service.json --start-streams --port=8021</code></pre>
<p>You can try the endpoints using in <a href="http://localhost:8021/api/docs">http://localhost:8021/api/docs</a></p>
<ul>
<li>First extract some valid customer_id and email using:
<pre><code>curl -X GET "http://localhost:8021/api/fraud-poc/0x0x1-service/test/find-orders?prefix=*&amp;num_items=10" \
-H "Accept: application/json"</code></pre>
This POC only can predict fraud for known customer_id and email in the generated data.</li>
</ul>
<p>Using a customer_id and email, pass a new order to the service using the Live: Predict Endpoint:
<img src="/fraud_poc/docs/img/api03.png" alt=""></p>
<p>And check the results, all calcualted features plus an is_fraud field is returned:</p>

<pre><code>{
  "order_id": "ce4798f5-6127-4d6e-bf1d-dda810eab26b",
  "order_date": "2020-07-07T06:33:18+00:00",
  "customer_id": "271d8c5e-e4e3-4377-a3e3-673ccf153664",
  "ip_addr": "f95e9c978b7f88dde5b9eb39417070251603db2d",
  "order_amount": 100.7097195892065,
  "email": "7545576ffe1b7c1d9d8d2e82d0191fa057df695f",
  "customer_id_by_email": [
    "271d8c5e-e4e3-4377-a3e3-673ccf153664"
  ],
  "num_customer_id_by_email": 1,
  "last_customer_id_by_email": "271d8c5e-e4e3-4377-a3e3-673ccf153664",
  "same_customer_id_by_email": 1,
  "known_customer_id_by_email": 1,
  "order_amount_mean_by_email": 468.79164250074143,
  "order_amount_std_by_email": 317.0635415216074,
  "order_amount_min_by_email": 68.2940660160266,
  "order_amount_max_by_email": 916.7097195892065,
  "order_amount_sum_by_email": 4687.916425007415,
  "order_amount_by_email": [
    769.0840886685221,
    68.2940660160266,
    164.22372869469348,
    198.35357128773578,
    454.66931470215576,
    100.7097195892065,
    779.1408217338134,
    916.7097195892065,
    854.4217419999278,
    382.3096527261267
  ],
  "key": "271d8c5e-e4e3-4377-a3e3-673ccf153664",
  "email_by_customer_id": [
    "7545576ffe1b7c1d9d8d2e82d0191fa057df695f"
  ],
  "ip_addr_by_customer_id": [
    "f95e9c978b7f88dde5b9eb39417070251603db2d",
    "788e574cf1934b34e9510ce897d8a593ab9dbcc9",
    "d02eae79264a401d76e853c41bdb781484443db2"
  ],
  "num_email_by_customer_id": 1,
  "num_ip_addr_by_customer_id": 3,
  "last_email_by_customer_id": "7545576ffe1b7c1d9d8d2e82d0191fa057df695f",
  "last_ip_addr_by_customer_id": "f95e9c978b7f88dde5b9eb39417070251603db2d",
  "same_email_by_customer_id": 1,
  "same_ip_addr_by_customer_id": 1,
  "known_email_by_customer_id": 1,
  "known_ip_addr_by_customer_id": 1,
  "order_amount_mean_by_customer_id": 468.79164250074143,
  "order_amount_std_by_customer_id": 317.0635415216074,
  "order_amount_min_by_customer_id": 68.2940660160266,
  "order_amount_max_by_customer_id": 916.7097195892065,
  "order_amount_sum_by_customer_id": 4687.916425007415,
  "order_amount_by_customer_id": [
    769.0840886685221,
    68.2940660160266,
    164.22372869469348,
    198.35357128773578,
    454.66931470215576,
    100.7097195892065,
    779.1408217338134,
    916.7097195892065,
    854.4217419999278,
    382.3096527261267
  ],
  "location_lat": 0,
  "location_long": 0,
  "is_fraud": 0.5424039363861084
}</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So that's it, please feel free to submit feedback and suggestions! Please contact me in case you want to improve pieces like dataset generation, model tuning, etc.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I Hope you enjoyed it!</p>

</div>
</div>
</div>
</div>
 

