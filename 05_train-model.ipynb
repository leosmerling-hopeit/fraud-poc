{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp model.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "> This step of the pipeline takes sampled data and uses Dask XGBoost implementation to train an XGBoost model for fraud classification. Model is then validated against validation dataset and copied to a place that can be use for the live fraud prediction service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import Dict, Optional\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "\n",
    "from hopeit.app.context import EventContext\n",
    "from hopeit.app.events import Spawn, SHUFFLE\n",
    "from hopeit.app.api import event_api\n",
    "from hopeit.app.logger import app_logger\n",
    "\n",
    "from fraud_poc.jobs import get_client, TrainModelJob, TrainingDataJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "__steps__ = ['setup', 'train_model', 'validate_model']\n",
    "\n",
    "logger = app_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def setup(job: TrainingDataJob, context: EventContext) -> TrainModelJob:\n",
    "    train_job = TrainModelJob(\n",
    "        train_data=job.sampled,\n",
    "        validation_data=job.validation,\n",
    "        model_path=context.env['model']['path']\n",
    "    )\n",
    "    logger.info(context, \"Setup {train_job}...\")\n",
    "    return train_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _features_and_labels(df):\n",
    "    X = df[['order_amount', \n",
    "            'num_email_by_customer_id', 'same_email_by_customer_id', 'known_email_by_customer_id', \n",
    "            'num_ip_addr_by_customer_id', 'same_ip_addr_by_customer_id', 'known_ip_addr_by_customer_id',\n",
    "            'num_customer_id_by_email', 'same_customer_id_by_email', 'known_customer_id_by_email',\n",
    "            'order_amount_mean_by_customer_id',\n",
    "            'order_amount_std_by_customer_id', 'order_amount_min_by_customer_id', 'order_amount_max_by_customer_id',\n",
    "            'order_amount_sum_by_customer_id', \n",
    "            'order_amount_mean_by_email',\n",
    "            'order_amount_std_by_email', 'order_amount_min_by_email', 'order_amount_max_by_email',\n",
    "            'order_amount_sum_by_email']]\n",
    "    y = df[['is_fraud']]\n",
    "    return X, y\n",
    "\n",
    "def _save_model(model, stats, path, version):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    file_name = f'{path}/{version}.pkl'\n",
    "    with open(file_name, 'wb') as fb:\n",
    "        pickle.dump(model, fb)\n",
    "    with open(f'{path}/{version}-stats.json', 'w') as fb:\n",
    "        json.dump(stats, fb)\n",
    "    return file_name\n",
    "\n",
    "def _load_model(file_name):\n",
    "    with open(file_name, 'rb') as fb:\n",
    "        return pickle.load(fb)\n",
    "\n",
    "def _score(y, y_pred, treshold=0.5):\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for v, p  in zip(y > treshold, y_pred > treshold):\n",
    "        if v and p: tp += 1\n",
    "        elif v and not p: fn += 1\n",
    "        elif not v and not p: tn += 1\n",
    "        elif not v and p: fp += 1\n",
    "\n",
    "\n",
    "    prec = tp/(tp+fp) if (tp+fp)>0 else 1.0\n",
    "    rec = tp/(tp+fn) if (tp+fn)>0 else 1.0\n",
    "    tnr = tn/(tn+fp) if (tn+fp)>0 else 1.0\n",
    "    acc = (tpr + tnr) / 2\n",
    "    f1 = 2*(prec*rec)/(prec+rec)\n",
    "\n",
    "    return {\n",
    "        'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn, 'prec': prec, 'rec': rec, 'tpr': rec, 'tnr': tnr, 'acc': acc, 'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train_model(job: TrainModelJob, context: EventContext) -> Optional[TrainModelJob]:\n",
    "    from dask_ml.model_selection import train_test_split\n",
    "    client = get_client()\n",
    "    try:\n",
    "        import dask_xgboost\n",
    "        logger.info(context, f\"Loading training data {job.train_data}...\")\n",
    "        df = dd.read_parquet(job.train_data, engine='fastparquet')\n",
    "        \n",
    "        X, y = _features_and_labels(df)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "        #XGBoost\n",
    "        logger.info(context, f\"Training XGBoost model...\")\n",
    "        params = {'objective': 'binary:logistic',\n",
    "                  'max_depth': 6, 'eta': 0.01,\n",
    "                  'min_child_weight': 0.5, 'verbosity': 1, 'tree_method': 'hist'}\n",
    "        evals = {}\n",
    "        model = dask_xgboost.train(client, params, X_train, y_train, \n",
    "                                 num_boost_round=10, evals_result=evals, eval_set=[(X_test.compute(), y_test.compute())])\n",
    "        job.model_path = _save_model(model, evals, context.env['model']['path'], str(uuid.uuid4()))\n",
    "        job.evals = evals\n",
    "        logger.info(context, f\"Saved model for validation in {job.model_path}\")\n",
    "        return job\n",
    "    except Exception as e:\n",
    "        logger.error(context, e)\n",
    "        return None\n",
    "    finally:\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def validate_model(job: TrainModelJob, context: EventContext) -> Optional[TrainModelJob]:\n",
    "    logger.info(context, \"Loading model for validation from {job.model_path}...\")\n",
    "    model = _load_model(job.model_path)\n",
    "    client = get_client()\n",
    "    try:\n",
    "        import dask_xgboost\n",
    "        logger.info(context, f\"Loading validation data from {job.validation_data}...\")\n",
    "        df = dd.read_parquet(job.validation_data, engine='fastparquet')\n",
    "        X, y = _features_and_labels(df)\n",
    "        \n",
    "        logger.info(context, f\"Predicting labels for validation...\")\n",
    "        y_pred = dask_xgboost.predict(client, model, X).persist()\n",
    "        y, y_pred = client.gather([y, y_pred])\n",
    "        score = _score(y.values.compute(), y_pred.compute())\n",
    "        logger.info(context, f'Validation score: {score}')\n",
    "        \n",
    "        min_score = context.env['model']['validation_min_score']\n",
    "        if score['f1'] >= min_score:\n",
    "            job.model_path = _save_model(model, score, context.env['model']['path'], 'latest-ok')\n",
    "            job.validation_score = score\n",
    "            logger.info(context, f\"Saved model for production in {job.model_path}\")\n",
    "            return job\n",
    "        logger.warning(context, f\"Model validation failed: score: {f1:.2f} / min_score: {min_score:.2f}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(context, e)\n",
    "        return None\n",
    "    finally:\n",
    "        client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test from notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-09 11:16:24,085 | INFO | fraud-poc 0.0.1-training model.train leos13 11247 | Setup {train_job}... | track.operation_id=test_operation_id | track.request_id=test_request_id | track.request_ts=2020-07-09T11:16:24.085488+00:00 | stream.name= | stream.msg_id= | stream.consumer_group=\n",
      "2020-07-09 11:16:25,763 | INFO | fraud-poc 0.0.1-training model.train leos13 11247 | Loading training data ./data/training/sampled/... | track.operation_id=test_operation_id | track.request_id=test_request_id | track.request_ts=2020-07-09T11:16:24.085488+00:00 | stream.name= | stream.msg_id= | stream.consumer_group=\n",
      "/opt/dev/anaconda3/envs/fraud-poc/lib/python3.7/site-packages/dask_ml/model_selection/_split.py:469: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  category=FutureWarning,\n",
      "2020-07-09 11:16:26,385 | INFO | fraud-poc 0.0.1-training model.train leos13 11247 | Training XGBoost model... | track.operation_id=test_operation_id | track.request_id=test_request_id | track.request_ts=2020-07-09T11:16:24.085488+00:00 | stream.name= | stream.msg_id= | stream.consumer_group=\n",
      "2020-07-09 11:16:30,306 | INFO | fraud-poc 0.0.1-training model.train leos13 11247 | Saved model for validation in ./data/model/xgb/b41cbd45-9d0e-46ef-83da-43262d51a34e.pkl | track.operation_id=test_operation_id | track.request_id=test_request_id | track.request_ts=2020-07-09T11:16:24.085488+00:00 | stream.name= | stream.msg_id= | stream.consumer_group=\n",
      "INFO:fraud-poc 0.0.1-training model.train leos13 11247:Saved model for validation in ./data/model/xgb/b41cbd45-9d0e-46ef-83da-43262d51a34e.pkl\n",
      "2020-07-09 11:16:31,876 | INFO | fraud-poc 0.0.1-training model.train leos13 11247 | Loading model for validation from {job.model_path}... | track.operation_id=test_operation_id | track.request_id=test_request_id | track.request_ts=2020-07-09T11:16:24.085488+00:00 | stream.name= | stream.msg_id= | stream.consumer_group=\n",
      "INFO:fraud-poc 0.0.1-training model.train leos13 11247:Loading model for validation from {job.model_path}...\n",
      "2020-07-09 11:16:32,968 | INFO | fraud-poc 0.0.1-training model.train leos13 11247 | Loading validation data from ./data/training/validation/... | track.operation_id=test_operation_id | track.request_id=test_request_id | track.request_ts=2020-07-09T11:16:24.085488+00:00 | stream.name= | stream.msg_id= | stream.consumer_group=\n",
      "INFO:fraud-poc 0.0.1-training model.train leos13 11247:Loading validation data from ./data/training/validation/...\n",
      "2020-07-09 11:16:33,004 | INFO | fraud-poc 0.0.1-training model.train leos13 11247 | Predicting labels for validation... | track.operation_id=test_operation_id | track.request_id=test_request_id | track.request_ts=2020-07-09T11:16:24.085488+00:00 | stream.name= | stream.msg_id= | stream.consumer_group=\n",
      "INFO:fraud-poc 0.0.1-training model.train leos13 11247:Predicting labels for validation...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/education/ml/dask/fraud-poc/fraud_poc/model/train.py\", line 129, in validate_model\n",
      "    score = _score(y.values.compute(), y_pred.compute())\n",
      "  File \"/home/education/ml/dask/fraud-poc/fraud_poc/model/train.py\", line 79, in _score\n",
      "    acc = (tpr + tnr) / 2\n",
      "NameError: name 'tpr' is not defined\n",
      "2020-07-09 11:16:35,651 | ERROR | fraud-poc 0.0.1-training model.train leos13 11247 | name 'tpr' is not defined | track.operation_id=test_operation_id | track.request_id=test_request_id | track.request_ts=2020-07-09T11:16:24.085488+00:00 | stream.name= | stream.msg_id= | stream.consumer_group= | trace=%5B%22Traceback%20%28most%20recent%20call%20last%29%3A%5Cn%22%2C%20%22%20%20File%20%5C%22/home/education/ml/dask/fraud-poc/fraud_poc/model/train.py%5C%22%2C%20line%20129%2C%20in%20validate_model%5Cn%20%20%20%20score%20%3D%20_score%28y.values.compute%28%29%2C%20y_pred.compute%28%29%29%5Cn%22%2C%20%22%20%20File%20%5C%22/home/education/ml/dask/fraud-poc/fraud_poc/model/train.py%5C%22%2C%20line%2079%2C%20in%20_score%5Cn%20%20%20%20acc%20%3D%20%28tpr%20%2B%20tnr%29%20/%202%5Cn%22%2C%20%22NameError%3A%20name%20%27tpr%27%20is%20not%20defined%5Cn%22%5D\n",
      "ERROR:fraud-poc 0.0.1-training model.train leos13 11247:name 'tpr' is not defined\n"
     ]
    }
   ],
   "source": [
    "from hopeit.testing.apps import config, execute_event\n",
    "\n",
    "app_config = config('config/training-pipeline.json')\n",
    "job = TrainingDataJob(sources={'customer_id': './data/features/customer_id/', 'email': './data/features/email/'}, \n",
    "                      sampled='./data/training/sampled/', validation='./data/training/validation/')\n",
    "\n",
    "result = await execute_event(app_config, 'model.train', job)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-929d8d894b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_tree\n",
    "\n",
    "bst = _load_model(result.model_path)\n",
    "plot_tree(bst, num_trees=1)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(100, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "\n",
    "ax = xgboost.plot_importance(bst, height=0.8, max_num_features=9)\n",
    "ax.grid(False, axis=\"y\")\n",
    "ax.set_title('Estimated feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_client()\n",
    "try:\n",
    "    import dask_xgboost\n",
    "    df = dd.read_parquet(result.validation_data, engine='fastparquet')\n",
    "    X, y = _features_and_labels(df)\n",
    "    y_hat = dask_xgboost.predict(client, bst, X).persist()\n",
    "    y, y_hat = client.gather([y, y_hat])\n",
    "    y, y_hat = y.values.compute(), y_hat.compute()\n",
    "finally:\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y, y_hat)\n",
    "\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(fpr, tpr, lw=3,\n",
    "        label='ROC Curve (area = {:.2f})'.format(auc(fpr, tpr)))\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "ax.set(\n",
    "    xlim=(0, 1),\n",
    "    ylim=(0, 1),\n",
    "    title=\"ROC Curve\",\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    ")\n",
    "ax.legend();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_score(y, y_hat, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
